{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 Deep learning basics and mathematical principles\n",
    "Deep learning is not as difficult as imagined, and even simpler than some traditional machine learning. The mathematical knowledge used does not need to be particularly advanced. This chapter will explain the basic theories in deep learning while implementing some simple theories through hands-on use of PyTorch. This chapter contains a lot of content, so only a brief introduction\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.1 Supervised learning and unsupervised learning\n",
    "Supervised learning, unsupervised learning, semi-supervised learning, and reinforcement learning are four common machine learning methods we come into contact with daily:\n",
    "\n",
    "-Supervised learning: Train to obtain an optimal model (this model belongs to a set of functions) through the existing training samples (ie, known data and its corresponding output). Best), and then use this model to map all inputs to corresponding outputs.\n",
    "-Unsupervised learning: The difference between it and supervised learning is that we do not have any training samples in advance, and we need to directly model the data.\n",
    "-Semi-supervised learning: Combine a large amount of unlabeled data and a small amount of labeled data in the training phase. Compared with the model using all label data, the training model using the training set can be more accurate during training.\n",
    "-Reinforcement learning: We set a reward function to confirm whether we are getting closer and closer to the goal. Similar to how we train a pet, we will reward him if we do it right, and we will punish him if we do it wrong. To achieve our training purpose.\n",
    "\n",
    "Here we only focus on supervised learning, because most of our courses later use supervised learning methods. The data input during training and verification contains both input x and output y corresponding to x, that is, learning data The correct answer has been given in advance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.2 Linear Regreesion\n",
    "Linear regression is a statistical analysis method that uses regression analysis in mathematical statistics to determine the quantitative relationship between two or more variables. It is widely used. Its expression is y = w'x+e, where e is a normal distribution whose error follows a mean value of 0.\n",
    "\n",
    "In regression analysis, only one independent variable and one dependent variable are included, and the relationship between the two can be approximated by a straight line. This kind of regression analysis is called unary linear regression analysis. If the regression analysis includes two or more independent variables, and the relationship between the dependent variable and the independent variable is linear, it is called multiple linear regression analysis.\n",
    "Excerpt from [Baidu Encyclopedia](https://baike.baidu.com/item/linear regression/8190345)\n",
    "\n",
    "simply put:\n",
    "Linear regression has a mapping f for input x and output y, y=f(x), and the form of f is aX+b. Among them, a and b are two adjustable parameters. When we train, we train the two parameters a and b.\n",
    "\n",
    "Let's use pyTorch code to do a detailed explanation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Quote\n",
    "# Note, here we use a new library called seaborn. If the package is not found, please use pip install seaborn to install\n",
    "import torch\n",
    "from torch.nn import Linear, Module, MSELoss\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "torch.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define a linear function. Here we use $y = 5x + 7$, where 5 and 7 are the parameters a and b mentioned above.\n",
    "Let’s use matplotlib to visualize this function first."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.linspace(0,20,500)\n",
    "y = 5*x + 7\n",
    "plt.plot(x,y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below I generate some random points as our training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.random.rand(256)\n",
    "noise = np.random.randn(256) / 4\n",
    "y = x * 5 + 7 + noise\n",
    "df = pd.DataFrame()\n",
    "df['x'] = x\n",
    "df['y'] = y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show the data we generated on the graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lmplot(x='x', y='y', data=df);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We randomly generated some points. Next, we will use PyTorch to build a linear model to fit them.\n",
    "This is the so-called training process. Since there is only one layer of linear model, we will use it directly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model=Linear(1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The parameter (1, 1) represents the number of input and output features (feature) is 1.\n",
    "The expression of the `Linear` model is $y=w \\cdot x+b$, where $w$ represents weight, and $b$ represents Bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss function we use the mean square loss function: `MSELoss`, this will be described in detail later"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimizer We choose the most common optimization method `SGD`, which is to calculate the gradient of `mini-batch`\n",
    "in each iteration, and then update the parameters with a learning rate of 0.01. The optimizer\n",
    "will also be introduced later in this chapter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optim = SGD(model.parameters(), lr = 0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train 3000 times"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 3000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare training data: The shape of `x_train`, `y_train` is (256, 1), which means that the size\n",
    "of `mini-batch` is 256, and the size of `feature` is 1. `astype('float32')`\n",
    "is for direct next step Convert to `torch.float`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train = x.reshape(-1, 1).astype('float32')\n",
    "y_train = y.reshape(-1, 1).astype('float32')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Started training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    # Sort the input and output data, where the input and output must be the Tensor type of torch\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    labels = torch.from_numpy(y_train)\n",
    "    #Use the model to predict\n",
    "    outputs = model(inputs)\n",
    "    #The gradient is set to 0, otherwise it will accumulate\n",
    "    optim.zero_grad()\n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    # Use the optimizer default method to optimize\n",
    "    optim.step()\n",
    "    if (i%100==0):\n",
    "        #Print the loss function every 100 times to see the effect\n",
    "        print('epoch {}, loss {:1.4f}'.format(i,loss.data.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training is complete, let's see what the results of the training are. Use `model.parameters()`\n",
    "to extract model parameters. $w$, $b$ are the model parameters we need to train\n",
    "The data we expect $w=5$, $b=7$ can be compared"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[w, b] = model.parameters()\n",
    "print (w.item(),b.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize our model again and see the data we trained. If you don’t like seaborn, you can use matplot directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted = model.forward(torch.from_numpy(x_train)).data.numpy()\n",
    "plt.plot(x_train, y_train,'go', label ='data', alpha = 0.3)\n",
    "plt.plot(x_train, predicted, label ='predicted', alpha = 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above is a simple example of using PyTorch to do linear regression.\n",
    "Below we will give a detailed introduction to the above content\n",
    "## 2.2.3 Loss Function\n",
    "The loss function is used to estimate the degree of inconsistency between the predicted value\n",
    "of the model (output in our example) and the true value (y_train in the example).\n",
    "It is a non-negative real valued function. The smaller the loss function, the smaller\n",
    "the model’s The better the robustness.\n",
    "The process of our training model is to use the optimization algorithm of gradient descent\n",
    "through continuous iterative calculations to make the loss function smaller and smaller.\n",
    "The smaller the loss function is, the better the algorithm is in the sense.\n",
    "\n",
    "Here is an important point: because PyTorch uses mini-batch to calculate,\n",
    "the calculated result of the loss function has been averaged over mini-batch\n",
    "\n",
    "Common (built-in PyTorch) loss functions are as follows:\n",
    "### nn.L1Loss:\n",
    "Enter the absolute value of the difference between x and target y.\n",
    "The dimensions of x and y are required to be the same (can be a vector or matrix),\n",
    "and the resulting loss dimensions are also corresponding to the same\n",
    "\n",
    "$ loss(x,y)=1/n\\sum|x_i-y_i| $\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.NLLLoss:\n",
    "Negative log-likelihood loss function for multiple classification\n",
    "\n",
    "$ loss(x, class) = -x[class]$\n",
    "\n",
    "If the weights parameter is passed in NLLLoss, the loss will be weighted, and the formula becomes\n",
    "\n",
    "$ loss(x, class) = -weights[class] * x[class] $"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.MSELoss:\n",
    "Mean square loss function, mean square error between input x and target y\n",
    "\n",
    "$ loss(x,y)=1/n\\sum(x_i-y_i)^2 $"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.CrossEntropyLoss:\n",
    "The cross entropy loss function for multi-classification, LogSoftMax and NLLLoss are integrated into one class,\n",
    "and the nn.NLLLoss function will be called, which we can understand as CrossEntropyLoss()=log_softmax() + NLLLoss()\n",
    "\n",
    "\n",
    "$\\begin{aligned} loss(x, class) &=\n",
    "-\\text{log}\\frac{exp(x[class])}{\\sum_j exp(x[j]))}\\ &= -x[class ] + log(\\sum_j exp(x[j])) \\end{aligned} $\n",
    "\n",
    "Because NLLLoss is used, the weight parameter can also be passed in.\n",
    "At this time, the calculation formula of loss becomes:\n",
    "\n",
    "$ loss(x, class) = weights[class] * (-x[class] + log(\\sum_j exp(x[j]))) $\n",
    "\n",
    "Therefore, this loss function is generally used in the case of multi-classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.BCELoss:\n",
    "Calculate the binary cross entropy between x and y.\n",
    "\n",
    "$ loss(o,t)=-\\frac{1}{n}\\sum_i(t[i]* log(o[i])+(1-t[i])* log(1-o[i] )) $\n",
    "\n",
    "Similar to NLLLoss, you can also add weight parameters:\n",
    "\n",
    "$ loss(o,t)=-\\frac{1}{n}\\sum_iweights[i]* (t[i]* log(o[i])+(1-t[i])* log(1- o[i])) $\n",
    "\n",
    "When using, you need to add the Sigmoid function in front of the layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.4 Gradient Descent\n",
    "When introducing the loss function, we have already said that gradient descent is an\n",
    "optimization algorithm that makes the loss function smaller and smaller. When there is\n",
    "no model parameter to solve the machine learning algorithm, that is, the constrained optimization problem,\n",
    "the gradient descent (Gradient Descent) is the most One of the commonly used methods.\n",
    "So gradient descent is the core of what we currently call machine learning.\n",
    "After understanding its meaning, you also understand the meaning of machine learning algorithms.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient\n",
    "In calculus, the partial derivative of the parameter of a multivariate function is obtained,\n",
    "and the partial derivative of each parameter obtained is written in the form of a vector, which is the gradient.\n",
    "For example, the function f(x,y), which takes the partial derivatives of x and y respectively,\n",
    "the obtained gradient vector is (∂f/∂x, ∂f/∂y)T, referred to as grad f(x,y) or ▽f (x,y)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.4 Gradient Descent\n",
    "When introducing the loss function, we have already said that gradient descent\n",
    "is an optimization algorithm that makes the loss function smaller and smaller.\n",
    "When there is no model parameter to solve the machine learning algorithm, that is,\n",
    "the constrained optimization problem, the gradient descent (Gradient Descent)\n",
    "is the most One of the commonly used methods. So gradient descent is the core of what we\n",
    "currently call machine learning. After understanding its meaning, you also understand\n",
    "the meaning of machine learning algorithms.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient\n",
    "In calculus, the partial derivative of the parameter of a multivariate function is obtained,\n",
    "and the partial derivative of each parameter obtained is written in the form of a vector, which is the gradient.\n",
    "For example, the function f(x,y), which takes the partial derivatives of x and y respectively,\n",
    "the obtained gradient vector is (∂f/∂x, ∂f/∂y)T, referred to as grad f(x,y) or ▽f (x,y).\n",
    "\n",
    "Geometrically speaking, the gradient is where the function change increases the fastest.\n",
    "Along the direction of the gradient vector, it is easier to find the maximum value of the function.\n",
    "Conversely, the gradient decreases the fastest along the opposite direction of the gradient vector,\n",
    "which means it is easier to find the minimum value of the function.\n",
    "\n",
    "We need to minimize the loss function, which can be solved step by step by the gradient descent method\n",
    "to obtain the minimized loss function and model parameter values.\n",
    "### Intuitive explanation of gradient descent method\n",
    "The gradient descent method is like going down the mountain. We don’t know the way down the mountain,\n",
    "so we decide to take one step and calculate one step. When we reach a position,\n",
    "we solve the gradient of the current position along the negative direction of the gradient,\n",
    "which is the steepest position Go down one step, and then continue to solve the current position gradient,\n",
    "and take a step along the steepest and easiest downhill position to this step.\n",
    "Going this way step by step, until we feel we have reached the foot of the mountain.\n",
    "\n",
    "As shown in the figure below, (this figure is taken from Baidu Encyclopedia)\n",
    "![](1.png)\n",
    "\n",
    "If we go on like this, it is possible that we cannot go to the foot of the mountain,\n",
    "but to the lower part of a certain local mountain peak (local optimal solution).\n",
    "\n",
    "This problem may be encountered in previous machine learning,\n",
    "because there are fewer features in machine learning,\n",
    "so it is likely to fall into a local optimal solution, but in deep learning,\n",
    "there are millions or even hundreds of millions of Features, the probability of this\n",
    "situation is almost 0, so we don't need to consider this problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mini-batch gradient descent method\n",
    "When performing gradient descent on the entire training set, we must process the entire training data\n",
    "set before performing one-step gradient descent, that is, each step of the gradient descent method needs\n",
    "to process the entire training set once, if the training data set is large, it will be processed\n",
    "The speed will be very slow, and it is impossible to load into memory or video memory all at once,\n",
    "so we will divide the large data set into small data sets, part of the training,\n",
    "this training subset is called Mini-batch.\n",
    "In PyTorch, this method is used for training. You can look at the introduction of dataloader\n",
    "in the previous chapter. The batch_size is the size of our Mini-batch.\n",
    "\n",
    "In order to make the introduction more concise, use the\n",
    "##link from aisummer\n",
    "[deeplearning.ai](https://www.deeplearning.ai/deep-learning-specialization/) course blackboard written by Wu Enda.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the ordinary gradient descent method, one epoch can only perform one gradient descent;\n",
    "for the Mini-batch gradient descent method, one epoch can perform several gradient descents of the Mini-batch.\n",
    "![](2.png)\n",
    "The change trend of the cost function of the ordinary batch gradient descent method and the\n",
    "Mini-batch gradient descent method is shown in the following figure:\n",
    "![](3.png)\n",
    "-If the size of the training sample is relatively small and can be read into the memory at once,\n",
    "then we don’t need to use Mini-batch,\n",
    "-If the size of the training sample is relatively large, it cannot be read into the memory at a time\n",
    "or is currently stored, then we must use Mini-batch to calculate in batches\n",
    "-The calculation rules of Mini-batch size are as follows, use 2 to the Nth power of the maximum\n",
    "size allowed by the memory\n",
    "![](4.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`torch.optim` is a library that implements various optimization algorithms. Most commonly used\n",
    "optimization algorithms have been implemented, and we can call them directly.\n",
    "### torch.optim.SGD\n",
    "Stochastic gradient descent algorithm, the algorithm with momentum (momentum) can be set as an optional\n",
    "parameter, for example:\n",
    "lr The parameter is the learning rate. For SGD, 0.1 0.01.0.001 is generally selected.\n",
    "How to set it will be explained in detail in the actual chapter\n",
    "If momentum is set, it is SGD with momentum, you don’t need to set it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### torch.optim.RMSprop\n",
    "In addition to the above Momentum gradient descent method with momentum,\n",
    "RMSprop (root mean square prop) is also an algorithm that can speed up gradient descent.\n",
    "Using the RMSprop algorithm, you can reduce the situation of large dimensional gradient\n",
    "update fluctuations and make the gradient The rate of decline becomes faster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Our course basically does not use RMSprop, so here is only an example\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### torch.optim.Adam\n",
    "The basic idea of ​​the Adam optimization algorithm is to combine Momentum and\n",
    "RMSprop to form an optimization algorithm suitable for different deep learning structures"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here lr, betas, and eps are all the default values, so Adam is the simplest optimization method to use\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.5 variance/bias\n",
    "-Deviation measures the deviation of the expected prediction of the learning algorithm from\n",
    "the true result, and immediately describes the fitting ability of the learning algorithm itself\n",
    "-Variance measures the changes in learning performance caused by changes in the training set\n",
    "of the same size, that is, the generalization ability of the model\n",
    "![](5.png)\n",
    "\n",
    "From the figure we can see\n",
    "-The situation of high bias is generally called underfitting, that is, our model does not\n",
    "fit the existing data well, and the fit is not enough.\n",
    "-The situation of high variance is generally called overfitting, that is, the model fits the\n",
    "training data too high and loses the ability to generalize.\n",
    "\n",
    "How to solve these two situations?\n",
    "\n",
    "Underfitting:\n",
    "-Increase the network structure, such as increasing the number of hidden layers;\n",
    "-Training longer;\n",
    "-Find a suitable network architecture and use a larger NN structure;\n",
    "\n",
    "Overfitting:\n",
    "-Use more data;\n",
    "-Regularization (regularization);\n",
    "-Find a suitable network structure;\n",
    "\n",
    "For example, in our example above, we can calculate our deviation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print (5-w.data.item(),7-b.data.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.6 Regularization\n",
    "Use regularization to solve the problem of high variance. Regularization is to add a regularization\n",
    "term to the Cost function to punish the complexity of the model. Here we briefly introduce\n",
    "the concept of regularization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### L1 regularization\n",
    "Add the absolute value of the weight parameter to the loss function\n",
    "\n",
    "$ L=E_{in}+\\lambda{\\sum_j} \\left|w_j\\right|$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### L2 regularization\n",
    "Add the sum of squares of the weight parameters to the loss function\n",
    "\n",
    "$ L=E_{in}+\\lambda{\\sum_j} w^2_j$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It should be noted that: l1 is easier to obtain sparse solutions than l2\n",
    "\n",
    "[Knowledge](https://www.zhihu.com/question/37096933/answer/70507353)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.4 Gradient Descent\n",
    "When introducing the loss function,\n",
    "we have already said that gradient descent is an optimization algorithm that makes the\n",
    "loss function smaller and smaller. When there is no model parameter to solve the machine\n",
    "learning algorithm, that is, the constrained optimization problem, the gradient descent\n",
    "(Gradient Descent) is the most One of the commonly used methods. So gradient descent is the core\n",
    "of what we currently call machine learning. After understanding its meaning, you also understand\n",
    "the meaning of machine learning algorithms.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient\n",
    "In calculus, the partial derivative of the parameter of a multivariate function is obtained,\n",
    "and the partial derivative of each parameter obtained is written in the form of a vector, which is the gradient.\n",
    "For example, the function f(x,y), which takes the partial derivatives of x and y respectively,\n",
    "the obtained gradient vector is (∂f/∂x, ∂f/∂y)T, referred to as grad f(x,y) or ▽f (x,y).\n",
    "\n",
    "Geometrically speaking, the gradient is where the function change increases the fastest.\n",
    "Along the direction of the gradient vector, it is easier to find the maximum value of the function.\n",
    "Conversely, the gradient decreases the fastest along the opposite direction of the gradient vector,\n",
    "which means it is easier to find the minimum value of the function.\n",
    "\n",
    "We need to minimize the loss function, which can be solved step by step by the gradient descent\n",
    "method to obtain the minimized loss function and model parameter values.\n",
    "### Intuitive explanation of gradient descent method\n",
    "The gradient descent method is like going down the mountain. We don’t know the way down the\n",
    "mountain, so we decided to take one step. When we reach a position, we solve the gradient\n",
    "of the current position along the negative direction of the gradient, which is the current\n",
    "steepest position. Go down one step, and then continue to solve the current position gradient,\n",
    "and take a step along the steepest and easiest downhill position to the position of this step.\n",
    "Going this way step by step, until we feel that we have reached the foot of the mountain.\n",
    "\n",
    "As shown in the figure below, (this figure is taken from Baidu Encyclopedia)\n",
    "![](1.png)\n",
    "\n",
    "If we continue this way, it is possible that we cannot go to the foot of the mountain,\n",
    "but to a certain local peak (local optimal solution).\n",
    "\n",
    "This problem may be encountered in previous machine learning, because there are fewer features in\n",
    "machine learning, so it is likely to fall into a local optimal solution, but in deep learning,\n",
    "there are millions or even hundreds of millions of Feature, the probability of this situation\n",
    "is almost 0, so we don’t need to consider this problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mini-batch gradient descent method\n",
    "When performing gradient descent on the entire training set, we must process the entire training data\n",
    "set before performing one-step gradient descent, that is, each step of the gradient descent method\n",
    "needs to process the entire training set once, if the training data set is large, it will be processed\n",
    "The speed will be very slow, and it is impossible to load into the memory or video memory at once,\n",
    "so we will divide the large data set into small data sets, part of the training, this training subset\n",
    "is called Mini-batch.\n",
    "In PyTorch, this method is used for training. You can see the batch_size in the introduction to\n",
    "the dataloader in the previous chapter is the size of our Mini-batch.\n",
    "\n",
    "In order to make the introduction more concise, use the\n",
    "[deeplearning.ai](https://www.deeplearning.ai/deep-learning-specialization/) course blackboard written by Wu Enda.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the ordinary gradient descent method, one epoch can only perform gradient descent;\n",
    "and for the Mini-batch gradient descent method, one epoch can perform multiple gradient descents of Mini-batch.\n",
    "![](2.png)\n",
    "The change trend of the cost function of the ordinary batch gradient descent method and\n",
    "the Mini-batch gradient descent method is shown in the following figure:\n",
    "![](3.png)\n",
    "-If the size of the training sample is relatively small and can be read into the memory at\n",
    "once, then we don’t need to use Mini-batch,\n",
    "-If the size of the training sample is relatively large, it cannot be read into the\n",
    "memory at a time or is currently stored, then we must use Mini-batch to calculate in batches\n",
    "-The calculation rules of Mini-batch size are as follows, use 2 to the Nth power of\n",
    "the maximum size allowed by the memory\n",
    "![](4.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`torch.optim` is a library that implements various optimization algorithms.\n",
    "Most commonly used optimization algorithms have been implemented, and we can call them directly.\n",
    "### torch.optim.SGD\n",
    "Stochastic gradient descent algorithm, the algorithm with momentum (momentum)\n",
    "can be set as an optional parameter, for example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#lr parameter is the learning rate. For SGD, 0.1 0.01.0.001 is generally selected. How to set it will be explained in detail in the actual chapter\n",
    "##If momentum is set, it is SGD with momentum, you don’t need to set it\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### torch.optim.RMSprop\n",
    "In addition to the above Momentum gradient descent method with momentum, RMSprop (root mean square prop)\n",
    "is also an algorithm that can speed up the gradient descent. Using the RMSprop algorithm, y\n",
    "ou can reduce the situation of large dimensional gradient update fluctuations and make the gradient The rate of decline becomes faster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Our course basically does not use RMSprop, so here is only an example\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### torch.optim.Adam\n",
    "The basic idea of ​​the Adam optimization algorithm is to combine Momentum and RMSprop to form an optimization algorithm suitable for different deep learning structures"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here lr, betas, and eps are all the default values, so Adam is the simplest optimization method to use\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.5 variance/bias\n",
    "-Deviation measures the deviation of the expected prediction of the learning algorithm from the true result, and immediately describes the fitting ability of the learning algorithm itself\n",
    "-Variance measures the changes in learning performance caused by changes in the training set of the same size, that is, the generalization ability of the model\n",
    "![](5.png)\n",
    "\n",
    "From the figure we can see\n",
    "-The condition of high bias is generally called underfitting, that is, our model does not fit the existing data well, and the fit is not enough.\n",
    "-The situation of high variance is generally called overfitting, that is, the model fits the training data too high and loses the ability to generalize.\n",
    "\n",
    "How to solve these two situations?\n",
    "\n",
    "Underfitting:\n",
    "-Increase the network structure, such as increasing the number of hidden layers;\n",
    "-Training longer;\n",
    "-Find a suitable network architecture and use a larger NN structure;\n",
    "\n",
    "Overfitting:\n",
    "-Use more data;\n",
    "-Regularization (regularization);\n",
    "-Find a suitable network structure;\n",
    "\n",
    "For example, in our example above, we can calculate our deviation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print (5-w.data.item(),7-b.data.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.6 Regularization\n",
    "Use regularization to solve the problem of high variance. Regularization is to add a regularization term to the Cost function to punish the complexity of the model. Here we briefly introduce the concept of regularization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### L1 regularization\n",
    "Add the absolute value of the weight parameter to the loss function\n",
    "\n",
    "$ L=E_{in}+\\lambda{\\sum_j} \\left|w_j\\right|$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### L2 regularization\n",
    "Add the sum of squares of the weight parameters to the loss function\n",
    "\n",
    "$ L=E_{in}+\\lambda{\\sum_j} w^2_j$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It should be noted that: l1 is easier to obtain sparse solutions than l2\n",
    "\n",
    "[Knowledge](https://www.zhihu.com/question/37096933/answer/70507353)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}