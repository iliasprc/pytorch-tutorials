{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Logisticr](./figures/1.8.logistic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "x =10.*(torch.rand(500)-0.5).numpy()\n",
    "y = (torch.rand(500)).numpy()\n",
    "\n",
    "line_x = 5 * torch.arange(-500,500).float()/500.0\n",
    "sigmoid_y = torch.sigmoid(line_x)\n",
    "\n",
    "#print(x[x > 0.5])\n",
    "pyplot.xlabel('X')\n",
    "pyplot.ylabel('Y')\n",
    "#pyplot.plot(x, y, 'bo')\n",
    "pyplot.axvline(0)\n",
    "pyplot.axhline(0)\n",
    "pyplot.grid(color='black', linestyle='-', linewidth=1)\n",
    "pyplot.plot(line_x , sigmoid_y,'r',linewidth=2, markersize=12)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.8 Introduction to logistic regression\n",
    "In this chapter, we will deal with structured data and use logistic regression to classify structured data simply.\n",
    "Logistic regression is a statistical model that adopts a logistic function to model a binary dependent\n",
    "variable, although many more complex extensions exist. In regression analysis, logistic regression\n",
    "is estimating the parameters of a logistic model in the form of binary regression.\n",
    "Mathematically, a binary logistic model has a dependent variable with two possible values,\n",
    "such as \"True/False\" or \"Yes/No\" which is represented by an indicator variable $p$ that is labeled with \"0\" or \"1\".\n",
    "Logistic regression is a kind of generalized linear regression (generalized linear model), which has many similarities\n",
    "with multiple linear regression analysis. Their model formulas are basically the same, both have $wx + \\beta$, where $w$ and\n",
    "$b$ are the parameters to be learned. The major difference lies in their different dependent variables, multiple linear\n",
    "regression directly uses $wx+\\beta$ as the dependent variable, that is, $y = wx+\\beta$. However, logistic regression uses the\n",
    "function $L$ to transform $wx+\\beta$ to a new variable $p$, $p = L(wx+\\beta)$ where the outcome variable  $p$ is dichotomous\n",
    "(it can take only two values). If $L$ is a logistic function, it is logistic regression while if $L$ is a polynomial\n",
    "function,it is polynomial regression.\n",
    "\n",
    "The general mathematical form of logistic regression is\n",
    "\n",
    "$l = \\log_{b}(\\frac{p}{1-p}) = w_{0} + w_{1}x_{1} + ... + w_{k}x_{k}$\n",
    "\n",
    "\n",
    "The base $b$ of the logarithm is usually taken equal to $e$.\n",
    "In general, logistic regression will add a layer of a non-linear function on top of a linear regression layer.\n",
    "Logistic regression is mainly for two-class prediction. We talked about the Sigmoid function in the activation function.\n",
    "The Sigmoid function is the most common logistic function, because the output of the Sigmoid function is the probability\n",
    "value between 0 and 1, when the probability is greater than 0.5 is predicted as 1, and less than 0.5 is predicted as 0.\n",
    " Sigmoid  is defined as follows:\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1+e^{(z)}}$\n",
    "\n",
    "\n",
    "Finally the probability $p$ can be defined as:\n",
    "\n",
    "$p = \\frac{1}{1 + e^{-({w_{0} + w_{1}x_{1} + ... + w_{k}x_{k})}}}$\n",
    "\n",
    "In addition, there is another type of logistic regression called Multinomial Logistic Regression\n",
    "in which the target variable has three or more possible values. But instead of sigmoid, softmax is adopted that\n",
    "calculated the probabilities among the desired output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5660]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+torch.exp(-z))\n",
    "\n",
    "w = torch.randn(1, 11, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "x = torch.randn(1,11)\n",
    "def logistic_model(x):\n",
    "    return sigmoid(x @ w.t() + b)\n",
    "\n",
    "\n",
    "p = logistic_model(x)\n",
    "print(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundary\n",
    "\n",
    "Since the sigmoid activation function outputs a value between 0 and 1, we must define a threshold\n",
    "to label our prediction as 0 or 1. The most common is to set a threshold at 0.5 where if the prediction\n",
    "is greater than 0.5, is labeled as 1 where if it's less than 0.5, it's labeled as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "decision = (p > 0.5).int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.8.2 UCI German Credit Data Set\n",
    "\n",
    "UCI German Credit is UCI's German credit data set, which contains original data and numerical data.\n",
    "The German Credit data is a data set that predicts the risk on loans based on personal\n",
    "information and overdue loan applications from customers. The data set contains 1000 pieces of data in 24 dimensions.\n",
    "This dataset classifies people described by a set of attributes as good or bad credit risks.\n",
    "Comes in two formats (one all numeric).\n",
    "Here we directly use the processed numerical data as a display.\n",
    "\n",
    "[Address](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and normalization\n",
    "Now we can load the data and we arte going to normalize each attribute by calculating\n",
    "the mean and standard deviation of each feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 24])\n"
     ]
    }
   ],
   "source": [
    "data=np.loadtxt(\"./data/german.data-numeric\")\n",
    "targets = torch.from_numpy(data)[:,-1][:,None].float()-1.\n",
    "data_tensor = torch.from_numpy(data)[:,:-1]\n",
    "means = torch.mean(data_tensor,dim=0)\n",
    "stds = torch.std(data_tensor,dim=0)\n",
    "data_tensor = ((data_tensor-means)/stds).float()\n",
    "print(targets.shape)\n",
    "print(data_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loaders construction\n",
    "Distinguish between the training set and the test set. Since there is no official verification set here, we can\n",
    "directly use the accuracy of the test set as the criterion to classify  a sample as  good or bad.\n",
    "\n",
    "Split dataset: 900 for training and 100 for testing\n",
    "\n",
    "The format of the dataset is that the first 24 columns are the attributes while the last one is the label\n",
    "(1 or 2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.9 *len(data_tensor))\n",
    "\n",
    "train_data = data_tensor[:split,:]\n",
    "train_targets = targets[:split,:]\n",
    "test_data = data_tensor[split:,:]\n",
    "test_targets = targets[split:,:]\n",
    "batch_size = 32\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "training_dataset = TensorDataset(train_data,train_targets)\n",
    "train_loader = DataLoader(training_dataset, batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(test_data,test_targets)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "#print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a logistic regression model\n",
    "Below we define the model, the model is very simple, a Linear layer (`nn.Linear()`)\n",
    " with a Sigmoid activation function (`nn.Sigmoid()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel,self).__init__()\n",
    "        self.model = nn.Linear(24,1) # Since 24 dimensions have been fixed, write 24 here\n",
    "        self.L_function = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        y = self.model(x)\n",
    "        p = self.L_function(y)\n",
    "        return p\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our model, optimizer and loss functions.\n",
    "It is a binary classification problem so we'll use `nn.BCELoss`\n",
    "$BCELoss(y,\\hat{y}) = -\\frac{1}{N}\\sum_{i=0}^{N}(y*\\log{\\hat{y}}+(1-y)*\\log{(1-\\hat{y})})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliasprc/Documents/penvs/venv/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "net = LogisticRegressionModel()\n",
    "criterion = nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "epochs = 100 # Number of epochs for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0 Loss 0.88 Batch = 28 0.35\n",
      "Training Epoch 1 Loss 0.86 Batch = 28 0.36\n",
      "Training Epoch 2 Loss 0.82 Batch = 28 0.39\n",
      "Training Epoch 3 Loss 0.80 Batch = 28 0.41\n",
      "Training Epoch 4 Loss 0.76 Batch = 28 0.45\n",
      "Training Epoch 5 Loss 0.74 Batch = 28 0.48\n",
      "Training Epoch 6 Loss 0.73 Batch = 28 0.51\n",
      "Training Epoch 7 Loss 0.71 Batch = 28 0.55\n",
      "Training Epoch 8 Loss 0.70 Batch = 28 0.58\n",
      "Training Epoch 9 Loss 0.68 Batch = 28 0.62\n",
      "Training Epoch 10 Loss 0.66 Batch = 28 0.64\n",
      "Training Epoch 11 Loss 0.65 Batch = 28 0.66\n",
      "Training Epoch 12 Loss 0.64 Batch = 28 0.67\n",
      "Training Epoch 13 Loss 0.63 Batch = 28 0.68\n",
      "Training Epoch 14 Loss 0.62 Batch = 28 0.69\n",
      "Training Epoch 15 Loss 0.61 Batch = 28 0.70\n",
      "Training Epoch 16 Loss 0.60 Batch = 28 0.71\n",
      "Training Epoch 17 Loss 0.60 Batch = 28 0.72\n",
      "Training Epoch 18 Loss 0.59 Batch = 28 0.73\n",
      "Training Epoch 19 Loss 0.58 Batch = 28 0.74\n",
      "Training Epoch 20 Loss 0.58 Batch = 28 0.74\n",
      "Training Epoch 21 Loss 0.58 Batch = 28 0.75\n",
      "Training Epoch 22 Loss 0.56 Batch = 28 0.75\n",
      "Training Epoch 23 Loss 0.55 Batch = 28 0.76\n",
      "Training Epoch 24 Loss 0.56 Batch = 28 0.76\n",
      "Training Epoch 25 Loss 0.55 Batch = 28 0.76\n",
      "Training Epoch 26 Loss 0.54 Batch = 28 0.77\n",
      "Training Epoch 27 Loss 0.55 Batch = 28 0.77\n",
      "Training Epoch 28 Loss 0.55 Batch = 28 0.77\n",
      "Training Epoch 29 Loss 0.54 Batch = 28 0.77\n",
      "Training Epoch 30 Loss 0.53 Batch = 28 0.77\n",
      "Training Epoch 31 Loss 0.54 Batch = 28 0.78\n",
      "Training Epoch 32 Loss 0.53 Batch = 28 0.78\n",
      "Training Epoch 33 Loss 0.52 Batch = 28 0.78\n",
      "Training Epoch 34 Loss 0.52 Batch = 28 0.77\n",
      "Training Epoch 35 Loss 0.52 Batch = 28 0.77\n",
      "Training Epoch 36 Loss 0.52 Batch = 28 0.78\n",
      "Training Epoch 37 Loss 0.52 Batch = 28 0.78\n",
      "Training Epoch 38 Loss 0.51 Batch = 28 0.78\n",
      "Training Epoch 39 Loss 0.51 Batch = 28 0.78\n",
      "Training Epoch 40 Loss 0.53 Batch = 28 0.78\n",
      "Training Epoch 41 Loss 0.51 Batch = 28 0.78\n",
      "Training Epoch 42 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 43 Loss 0.51 Batch = 28 0.78\n",
      "Training Epoch 44 Loss 0.51 Batch = 28 0.78\n",
      "Training Epoch 45 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 46 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 47 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 48 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 49 Loss 0.52 Batch = 28 0.78\n",
      "Training Epoch 50 Loss 0.51 Batch = 28 0.78\n",
      "Training Epoch 51 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 52 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 53 Loss 0.51 Batch = 28 0.78\n",
      "Training Epoch 54 Loss 0.50 Batch = 28 0.79\n",
      "Training Epoch 55 Loss 0.50 Batch = 28 0.79\n",
      "Training Epoch 56 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 57 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 58 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 59 Loss 0.52 Batch = 28 0.78\n",
      "Training Epoch 60 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 61 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 62 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 63 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 64 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 65 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 66 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 67 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 68 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 69 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 70 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 71 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 72 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 73 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 74 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 75 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 76 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 77 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 78 Loss 0.48 Batch = 28 0.79\n",
      "Training Epoch 79 Loss 0.52 Batch = 28 0.79\n",
      "Training Epoch 80 Loss 0.49 Batch = 28 0.79\n",
      "Training Epoch 81 Loss 0.49 Batch = 28 0.79\n",
      "Training Epoch 82 Loss 0.48 Batch = 28 0.79\n",
      "Training Epoch 83 Loss 0.48 Batch = 28 0.79\n",
      "Training Epoch 84 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 85 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 86 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 87 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 88 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 89 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 90 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 91 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 92 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 93 Loss 0.49 Batch = 28 0.78\n",
      "Training Epoch 94 Loss 0.50 Batch = 28 0.78\n",
      "Training Epoch 95 Loss 0.48 Batch = 28 0.78\n",
      "Training Epoch 96 Loss 0.48 Batch = 28 0.79\n",
      "Training Epoch 97 Loss 0.49 Batch = 28 0.79\n",
      "Training Epoch 98 Loss 0.52 Batch = 28 0.79\n",
      "Training Epoch 99 Loss 0.49 Batch = 28 0.79\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    # Specify the model as training mode and calculate the gradient\n",
    "    net.train()\n",
    "    # Input values need to be converted into torch Tensor\n",
    "    avg_loss = 0.\n",
    "    total_samples =0.\n",
    "    correct_predictions = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "         #Clear the loss of the previous step\n",
    "        optimizer.zero_grad()\n",
    "        y_hat=net(data)\n",
    "        loss=criterion(y_hat,target) # calculate loss\n",
    "        avg_loss+=loss.item()\n",
    "        pred_label = (y_hat>0.5).int()\n",
    "        #print(pred_label)\n",
    "        total_samples += data.size()[0]\n",
    "        correct_predictions += (pred_label == target).sum()\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # optimization\n",
    "        #if (batch_idx+1) % 100 == 0: # Here we output relevant information every 100 times\n",
    "    print(f'Training Epoch {i} Loss {avg_loss/batch_idx:.2f} Batch = {batch_idx} {correct_predictions/total_samples:.2f}')\n",
    "        # Specify the model as calculation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is complete, let's evaluate now the logistic regression model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 0.48 0.79 \n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "avg_loss = 0.\n",
    "total_samples =0.\n",
    "correct_predictions = 0.\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        y_hat=net(data)\n",
    "        loss=criterion(y_hat,target) # calculate loss\n",
    "        avg_loss+=loss.item()\n",
    "        pred_label = (y_hat>0.5).int()\n",
    "        total_samples += data.size()[0]\n",
    "        correct_predictions += (pred_label == target).sum()\n",
    "    print(f'Test Loss {avg_loss/len(test_loader):.2f} {(correct_predictions/total_samples):.2f} ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "#TODO Multinomial regression\n",
    "\n",
    "\n",
    "\n",
    "That was a basic introduction to the logistic regression.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}